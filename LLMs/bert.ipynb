{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all of the dogwhistle expressions in the original data \n",
    "# are analysed. \n",
    "\n",
    "ignore = tuple([\"P1_\", \"V1_hjälpa\", \"V2_hjälpa\", \"X_hjälpa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_split(string):\n",
    "    k, v = (f\"{string.split(' -> ')[1]}_{string.split()[-1]}\", string.split(' -> ')[2])\n",
    "    return k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"dwts.paradigm.txt\", encoding=\"utf-8\") as f:\n",
    "    paradigms = [line.strip(\"\\n\") for line in f.readlines() if not line.startswith(\"#\")]\n",
    "paradigms = [p for p in paradigms if p != \"\"]\n",
    "paradigms = [p.split(\" #\")[0] for p in paradigms]\n",
    "paradigms = dict([p_split(paradigm) for paradigm in paradigms])\n",
    "print(paradigms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(sentence, exact_match, lemma, model, device = \"cpu\", only_check = True):\n",
    "    if lemma.startswith(\"X\"): # X_globalist\n",
    "        true_lemma = lemma.split(\"_\")[-1] \n",
    "        true_wf = true_lemma + exact_match.split(true_lemma)[-1]\n",
    "    else: \n",
    "        if lemma.endswith(\"X\"): # N1C_globalistX\n",
    "            true_lemma = lemma.split(\"_\")[1][:-1]\n",
    "            true_wf = lemma.split(\"_\")[1][:-1] #true_lemma\n",
    "        else: # N1_globalist\n",
    "            true_lemma = lemma.split(\"_\")[1]\n",
    "            true_wf = exact_match\n",
    "\n",
    "    encoded = tokenizer.encode_plus(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    tokens = [tokenizer.decode(wid) for wid in encoded[\"input_ids\"][0]]\n",
    "\n",
    "    try:\n",
    "        idx = sentence.split().index(exact_match) # will not match tokenizer; hence `map_tok()`\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    token_ids_word = np.where(np.array(encoded.word_ids()) == idx)[0]\n",
    "    \n",
    "    if lemma.endswith(\"X\") or lemma.startswith(\"X\"):\n",
    "        start_with = min(token_ids_word)\n",
    "        outer = start_with\n",
    "        top    = 0\n",
    "\n",
    "        if lemma.startswith(\"X\"):\n",
    "            for i, idx in enumerate(token_ids_word):\n",
    "                if true_lemma.startswith(tokens[idx].replace(\"##\", \"\")):\n",
    "                    token_ids_word = token_ids_word[i:]\n",
    "                    start_with = min(token_ids_word)\n",
    "                    break\n",
    "\n",
    "        for i in token_ids_word: \n",
    "            i = i + 1\n",
    "            candidate = \"\".join([tok.replace(\"##\", \"\") for tok in tokens[min(token_ids_word):i]])\n",
    "            score = SequenceMatcher(None, true_wf, candidate).ratio()\n",
    "            if score >= top:\n",
    "                top = score\n",
    "                outer = i\n",
    "\n",
    "        token_ids_word = np.arange(start_with, outer) # arrange\n",
    "\n",
    "    if only_check:\n",
    "        tokens = tokens[token_ids_word[0]:token_ids_word[-1]+1]\n",
    "        tokens_short = \"\".join([tok.replace(\"##\", \"\") for tok in tokens])\n",
    "        if not \"hjälpa_på\" in lemma:\n",
    "            if tokens_short != true_wf:\n",
    "                if sum([dwe in tokens_short for dwe in [\"globalist\", \"berika\", \"återvandr\", \"förortsgäng\"]]) == 0:\n",
    "                    print(f\"{lemma} | {true_lemma} | {exact_match} | {true_wf}  >>> {' '.join(tokens)} <<<  ({token_ids_word}):\\n1:{sentence}\")#\\n2:{inbetween}\")\n",
    "        return\n",
    "\n",
    "    encoded.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "\n",
    "    last_hidden = output.last_hidden_state.squeeze()\n",
    "    word_tokens_output = last_hidden[token_ids_word]\n",
    "\n",
    "    return word_tokens_output.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handles preprocessing so that tokenization match token index. \n",
    "\n",
    "def map_tok(sentence):\n",
    "    sentence = sentence.replace(\"-\", \" - \")\n",
    "    sentence = sentence.replace(\".\", \" . \")\n",
    "    sentence = sentence.replace(\"+\", \" + \")\n",
    "    sentence = sentence.replace(\"&\", \" & \")\n",
    "    sentence = sentence.replace(\":\", \" : \")\n",
    "    sentence = sentence.replace(\"*\", \" * \")\n",
    "    sentence = sentence.replace(\"^\", \" ^ \")\n",
    "    sentence = sentence.replace(\"ü\", \"u\")\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"=) \", \"\")\n",
    "    sentence = sentence.replace(\">= \", \"\")\n",
    "    sentence = sentence.replace(\"=>\", \"\")\n",
    "    sentence = sentence.replace(\">>\", ' \" ')\n",
    "    sentence = sentence.replace(\"<<\", ' \" ')\n",
    "    sentence = sentence.replace(\"| \", \"\")\n",
    "    sentence = re.sub(r\"([a-zåäö])(['`])([a-zåäö])\", r\"\\1 \\2 \\3\", sentence)\n",
    "    sentence = re.sub(r\"([)\\?=%!<>~«])([a-zåäö0-9])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([a-zåäö0-9])([\\))\\?=%!<>~«])\", r\"\\1 \\2\", sentence)\n",
    "    sentence = re.sub(r\"([0-9]),([0-9])\", r\"\\1 , \\2\", sentence)\n",
    "    sentence = re.sub(r\"#+\", \" * \", sentence)\n",
    "    sentence = re.sub(r\"([=¤])+\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r\" +\", \" \", sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(\n",
    "    model,\n",
    "    directory,\n",
    "    vector_dir,\n",
    "    paradigms,\n",
    "    ignore,\n",
    "    device=\"cpu\",\n",
    "    only_check = True,\n",
    "    re_start = None\n",
    "):\n",
    "\n",
    "    directory = Path(directory)\n",
    "    vector_dir = Path(vector_dir)\n",
    "    files = os.listdir(directory)\n",
    "    if re_start != None:\n",
    "        years = sorted([int(y.replace(\".txt\", \"\")) for y in files])\n",
    "        files = [f\"{y}.txt\" for y in years if y >= re_start]\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for file in files:\n",
    "        print()\n",
    "        print(file)\n",
    "        with open(directory / file, encoding=\"utf-8\") as f, open(vector_dir / file, \"w\", encoding=\"utf-8\") as out:\n",
    "            for i, line in enumerate(f):\n",
    "                if i % 10 == 0:\n",
    "                    print(i, end=\"\\r\")\n",
    "                lemma, n, sentence = tuple(line.strip(\"\\n\").split(\"\\t\"))\n",
    "\n",
    "                sentence = map_tok(sentence)\n",
    "\n",
    "                if int(n) == 1: \n",
    "                    if lemma.startswith(ignore):\n",
    "                        continue\n",
    "                    if lemma in paradigms:\n",
    "                        regex = paradigms[lemma]\n",
    "                        regex = re.compile(regex)    \n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"||\", sentence)\n",
    "                        exact_match = exact_match.group()                        \n",
    "                    else:\n",
    "                        regex = re.compile(f\"\\\\b[0-9a-zåäö]*{lemma.split('_')[-1]}.*?\\\\b\")\n",
    "                        exact_match = re.search(regex, sentence)\n",
    "                        if exact_match == None:\n",
    "                            print(\"ERROR:\", lemma, \"|\", regex, \"|\", sentence)\n",
    "                        exact_match = exact_match.group()\n",
    "                    vector = get_word_vector(sentence, exact_match, lemma, model, device, only_check)\n",
    "                    if only_check or vector == None:\n",
    "                        continue\n",
    "                        \n",
    "                    vector = \" \".join([str(v) for v in vector.tolist()]) \n",
    "                    out.write(f\"{lemma}\\t{vector}\\n\")                   \n",
    "\n",
    "                else: # two instances of the same lemma = problem\n",
    "                    for l in lemma.split(\"; \"):\n",
    "                        if l.startswith(ignore):\n",
    "                            continue                        \n",
    "                        if l in paradigms:\n",
    "                            regex = paradigms[l]\n",
    "                            regex = re.compile(regex)    \n",
    "                            exact_match = re.search(regex, sentence).group() \n",
    "                        else:\n",
    "                            regex = re.compile(f\"\\\\b[0-9a-zåäö]*{l.split('_')[-1]}.*?\\\\b\")\n",
    "                            exact_match = re.search(regex, sentence).group()\n",
    "                        vector = get_word_vector(sentence, exact_match, l, model, device, only_check)\n",
    "                        if only_check or vector == None:\n",
    "                            continue\n",
    "                        vector = \" \".join([str(v) for v in vector.tolist()])\n",
    "                        out.write(f\"{l}\\t{vector}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'KB/bert-base-swedish-cased' # Change to prefered HuggingFace path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "short_name = model_name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "get_word_embeddings(\n",
    "    model=model, \n",
    "    directory=Path(\"../data/corpus/fb_pol_files/\"), \n",
    "    vector_dir=f\"../data/vectors/fb_pol/{short_name}\", \n",
    "    paradigms=paradigms,\n",
    "    ignore=ignore,\n",
    "    device=\"cuda\", \n",
    "    only_check = False, # Provide True if you only want to check mapping of tokenization and token indices\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
