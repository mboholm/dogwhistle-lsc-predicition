{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sk_cos\n",
    "import pandas as pd\n",
    "import random\n",
    "import tracemalloc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_cos(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_distance(v1, v2):\n",
    "    return np.arccos(np_cos(v1,v2)) / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upc_mean0(lst, function): # Mean of Unique Pairwise Comparison \n",
    "    # Very ineffecient\n",
    "    \n",
    "    if len(lst) == 1:\n",
    "        return np.nan\n",
    "    \n",
    "    total = []\n",
    "\n",
    "    for i, x in enumerate(lst, start=1): # Obs! start = 1\n",
    "        for y in lst[i:]:\n",
    "            total.append(function(x, y)) \n",
    "    \n",
    "    M = sum(total)/len(total)\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upc_mean(lst, function=sk_cos): # Mean of Unique Pairwise Comparison (UPC)\n",
    "    \n",
    "    if len(lst) == 1:\n",
    "        return np.nan    \n",
    "    \n",
    "    matrix = function(lst)\n",
    "    \n",
    "    L    = len(lst)\n",
    "    S    = matrix.sum()\n",
    "    adjS = S - L    # Remove diagonal ; 1.0 x N\n",
    "    adjS = adjS / 2 # Remove duplicates from upper AND lower half in matrix\n",
    "    N    = ((L*L) - L) / 2\n",
    "    \n",
    "    M = adjS / N \n",
    "    \n",
    "    return M\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iline(line):\n",
    "    line = line.strip(\"\\n\")\n",
    "    line = line.split(\"\\t\")\n",
    "    term = line[0]\n",
    "    if len(term.split()) > 1:\n",
    "        term = \"_\".join(term.split()[:3])\n",
    "    vector = [float(value) for value in line[-1].split()]\n",
    "    return term, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(vectors):\n",
    "    arr = np.array(vectors)\n",
    "    return np.mean(arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def status_report(memory0, t0, prefix=\"\"):\n",
    "    norm, unit = (1000000, \"MB\")\n",
    "    memory1 = tracemalloc.get_traced_memory()\n",
    "    memory  = round(memory1[0]/norm, 1)\n",
    "    memory_delta = round((memory1[0]-memory0[0])/norm, 1)\n",
    "    memory0 = memory1\n",
    "    t_delta = time.time() - t0\n",
    "    m = int(t_delta/60)\n",
    "    s = int(t_delta%60)\n",
    "    print(f\"#{prefix}--memory={memory} {unit}; {m} m {s} s.                   \", end=\"\\r\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(year, term, directory):\n",
    "    \n",
    "    filename = f\"{term}_{year}.tmp\" # f\"{term}_{this_year['year']}.tmp\"\n",
    "    vs = []\n",
    "    with open(directory / filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            vs.append(np.fromstring(line, dtype=float, sep=' '))\n",
    "    \n",
    "    return vs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merger = {\n",
    "    \"N1C_berikareX\": \"N1_berikare\",\n",
    "    \"N1C_förortsgängX\": \"N1_förortsgäng\",\n",
    "    \"N1C_globalistX\": \"N1_globalist\",\n",
    "    \"N1C_kulturberikarX\": \"N1_kulturberikare\",\n",
    "    \"N1C_återvandringsX\": \"N1_återvandring\",\n",
    "    \"N2C_återvandrarX\": \"N2_återvandrare\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_change(\n",
    "    model, \n",
    "    out_dir, \n",
    "    n_controls, \n",
    "    pool_by = centroid, \n",
    "    merge_cmp = False, # Merge compounds, e.g. N1C_berikareX --> N1_berikare (there is a risk BERT clustering picks up on this)\n",
    "    x_help = True, # Ad hoc! Should be solved in the data!! E.g. \"X_hjälpa dem på plats\" should be solved by paradigm file\n",
    "    tmp_dir = \"/home/max/tmp/tmp\"\n",
    "):\n",
    "\n",
    "    tracemalloc.start()\n",
    "    memory0 = (0,0)\n",
    "    t0 = time.time()    \n",
    "\n",
    "    model = Path(model)\n",
    "    tmp_dir = Path(tmp_dir)\n",
    "    files = os.listdir(model)\n",
    "    files.sort()\n",
    "    previous_year = {}\n",
    "    \n",
    "    for i, file in enumerate(files, start=1):\n",
    "        print()\n",
    "        print(model, file)\n",
    "        this_year = {\"year\": file.replace(\".txt\", \"\"), \"data\": {}}\n",
    "\n",
    "        # Get data\n",
    "        with open(model/file, \"r\") as f:\n",
    "            for j, line in enumerate(f):\n",
    "\n",
    "                if j % 300 == 0:\n",
    "                    status_report(memory0, t0, prefix=f\"Line {j} of file\")\n",
    "\n",
    "                term, vector = iline(line)\n",
    "                \n",
    "                if merge_cmp:\n",
    "                    if term in merger:\n",
    "                        term = merger[term]\n",
    "                \n",
    "                if x_help:  # Should be solved in the data by dwts.paradigm\n",
    "                    if term.startswith(\"x_hjälpa\"):\n",
    "                        term = \"x_hjälpa\"\n",
    "                \n",
    "                if term == \"\":\n",
    "                    continue\n",
    "                if term in this_year[\"data\"]:\n",
    "                    this_year[\"data\"][term][\"vector\"].append(vector)\n",
    "                else:\n",
    "                    this_year[\"data\"][term] = {\"vector\": [vector]}\n",
    "\n",
    "        status_report(memory0, t0, prefix=\"Input done\")\n",
    "        \n",
    "        #print(\"terms:\", sorted(this_year[\"data\"].keys()))\n",
    "\n",
    "        # Pool, Spread\n",
    "        with open(out_dir/f\"centroid/{file}\", \"w\") as emb, open(out_dir/f\"spread/{file}\", \"w\") as wdt: \n",
    "            for term in sorted(this_year[\"data\"].keys()):\n",
    "                \n",
    "                status_report(memory0, t0, prefix=f\"Pool, Spread: {term}\")\n",
    "\n",
    "                pool = pool_by(this_year[\"data\"][term][\"vector\"])\n",
    "                status_report(memory0, t0, prefix=f\"Pool - check!\")\n",
    "                # high spread close to 1 (similarity low); low spread close to 0 (similarity high)\n",
    "                spread = 1 - upc_mean(this_year[\"data\"][term][\"vector\"]) \n",
    "                status_report(memory0, t0, prefix=f\"UPC - check!\")\n",
    "                \n",
    "                with open(tmp_dir / f\"{term}_{this_year['year']}.tmp\", \"w\") as f:\n",
    "#                     print()\n",
    "#                     print(term, this_year[\"data\"][term][\"vector\"])\n",
    "#                     print()\n",
    "                    for v in this_year[\"data\"][term][\"vector\"]:\n",
    "                        #print(v)\n",
    "                        #f.write(np.array_repr(v))\n",
    "                        f.write(\" \".join([str(val) for val in v]) + \"\\n\")\n",
    "                \n",
    "                del this_year[\"data\"][term][\"vector\"]\n",
    "                \n",
    "                this_year[\"data\"][term][\"mean\"] = pool\n",
    "                pool_as_str = \" \".join([str(v) for v in pool.tolist()])# consider: np.array_repr(pool).replace(\"\\n\", \"\"), but we do not want brackets\n",
    "                emb.write(f\"{term}\\t{pool_as_str}\\n\") \n",
    "\n",
    "                status_report(memory0, t0, prefix=f\"Starting UPC: {term}\")\n",
    "\n",
    "                \n",
    "                ################  TO BE REMOVED   ##################\n",
    "                if spread > 1.000001: \n",
    "                    # There is some rounding issue ... Noble et al observed something similar; identical vectors have cos_sin > 1.0\n",
    "                    \n",
    "                    print()\n",
    "                    print(f\"Something went wrong! ({term})\")\n",
    "                    print(\"Spread =\", spread)\n",
    "                    \n",
    "                    #vecs = this_year[\"data\"][term][\"vector\"]\n",
    "                    #L    = len(vecs)\n",
    "                    #print(\"L\", L)\n",
    "                    #mtrx = sk_cos(vecs)\n",
    "                    #df   = pd.DataFrame(mtrx)\n",
    "                    #upc0 = upc_mean0(vecs, function=np_cos)\n",
    "                    \n",
    "                    #print(\"UPC0:\", upc0)\n",
    "                    #print(df.round(3))\n",
    "                    #print(\"Matrix sum\", mtrx.sum())\n",
    "                    #print(\"Adjusted Sum\", (mtrx.sum() - len(vecs))/2)\n",
    "                    #print(\"Adjusted N\", ((L*L) - L) / 2)\n",
    "                    print()\n",
    "                    \n",
    "                    #return\n",
    "                #####################################################\n",
    "                this_year[\"data\"][term][\"spread\"] = spread\n",
    "                wdt.write(f\"{term}\\t{spread}\\n\")\n",
    "\n",
    "        status_report(memory0, t0, prefix=\"Centroids, Spread done\")\n",
    "\n",
    "        # Change and controls\n",
    "        if i > 1:\n",
    "            yi = previous_year[\"year\"]\n",
    "            yj = this_year[\"year\"]\n",
    "            \n",
    "            shared_terms = [term for term in this_year[\"data\"].keys() if term in previous_year[\"data\"].keys()]\n",
    "\n",
    "            # Genuine change\n",
    "            with open(out_dir/f\"cosine_change/{yi}_{yj}_genuine.txt\", \"w\") as f:\n",
    "                for term in shared_terms:\n",
    "                    gch = angular_distance(previous_year[\"data\"][term][\"mean\"], this_year[\"data\"][term][\"mean\"])\n",
    "                    f.write(f\"{term}\\t{gch}\\n\")\n",
    "\n",
    "            # Genuine similarity\n",
    "            with open(out_dir/f\"cosine_sim/{yi}_{yj}_genuine.txt\", \"w\") as f:\n",
    "                for term in shared_terms:\n",
    "                    sim = np_cos(previous_year[\"data\"][term][\"mean\"], this_year[\"data\"][term][\"mean\"])\n",
    "                    f.write(f\"{term}\\t{sim}\\n\")\n",
    "\n",
    "            status_report(memory0, t0, prefix=\"Start Control\")\n",
    "\n",
    "            # Controls\n",
    "            control = {c: {} for c in range(1, n_controls+1)}\n",
    "            for c in range(1, n_controls+1):\n",
    "                status_report(memory0, t0, prefix=f\"Control: {c}\")\n",
    "                for term in shared_terms:\n",
    "                    control[c][term]={}\n",
    "                    \n",
    "                    mega = []\n",
    "                    mega.extend(get_vector(year = yi, term = term, directory = tmp_dir))\n",
    "                    mega.extend(get_vector(year = yj, term = term, directory = tmp_dir))\n",
    "                    \n",
    "                    random.shuffle(mega)\n",
    "\n",
    "                    cutoff = int(len(mega)/2)\n",
    "                    ctrl1 = mega[:cutoff]\n",
    "                    ctrl2 = mega[cutoff:]\n",
    "                    mean1 = pool_by(ctrl1)\n",
    "                    mean2 = pool_by(ctrl2)\n",
    "\n",
    "                    # can you calculate rectified value for spread?\n",
    "\n",
    "                    control[c][term][\"cch\"] = angular_distance(mean1, mean2)\n",
    "                    control[c][term][\"csim\"] = np_cos(mean1, mean2)\n",
    "                    \n",
    "                    status_report(memory0, t0)\n",
    "\n",
    "                    del mega\n",
    "                    \n",
    "                    status_report(memory0, t0)\n",
    "\n",
    "            #status_report(memory0, t0)\n",
    "\n",
    "            for n in control.keys():\n",
    "                with open(out_dir/f\"cosine_change/{yi}_{yj}_control{n}.txt\", \"w\") as f:\n",
    "                    for term in control[n].keys():\n",
    "                        cch = control[n][term][\"cch\"]\n",
    "                        f.write(f\"{term}\\t{cch}\\n\")\n",
    "\n",
    "                with open(out_dir/f\"cosine_sim/{yi}_{yj}_control{n}.txt\", \"w\") as f:\n",
    "                    for term in control[n].keys():\n",
    "                        sim = control[n][term][\"csim\"]\n",
    "                        f.write(f\"{term}\\t{sim}\\n\")                            \n",
    "\n",
    "        previous_year = {k:v for k,v in this_year.items()}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple(models, results, n_controls, merge_compounds, tmp):\n",
    "    \n",
    "    for model in os.listdir(models):\n",
    "        model = Path(model)\n",
    "        semantic_change(\n",
    "            model = models / model, \n",
    "            out_dir = results / model, \n",
    "            n_controls = n_controls,\n",
    "            merge_cmp = merge_compounds, \n",
    "            tmp_dir=tmp\n",
    "        )\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2000.txt\n",
      "#Centroids, Spread done--memory=0.5 MB; 0 m 1 s.                            \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2001.txt\n",
      "#--memory=0.8 MB; 0 m 4 s.                                                  \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2002.txt\n",
      "#--memory=1.3 MB; 0 m 7 s.                                                  \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2003.txt\n",
      "#--memory=0.7 MB; 0 m 9 s.                                                  \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2004.txt\n",
      "#--memory=0.7 MB; 0 m 13 s.                                                  \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2005.txt\n",
      "#--memory=0.9 MB; 0 m 21 s.                                                   \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2006.txt\n",
      "#--memory=1.1 MB; 0 m 38 s.                                                     \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2007.txt\n",
      "#--memory=1.1 MB; 1 m 22 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2008.txt\n",
      "#--memory=1.1 MB; 3 m 14 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2009.txt\n",
      "#--memory=1.1 MB; 5 m 44 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2010.txt\n",
      "#--memory=1.1 MB; 8 m 50 s.                                                     \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2011.txt\n",
      "#--memory=1.2 MB; 11 m 38 s.                                                     \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2012.txt\n",
      "#--memory=2.4 MB; 14 m 28 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2013.txt\n",
      "#--memory=1.1 MB; 17 m 4 s.                                                       \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2014.txt\n",
      "#--memory=1.4 MB; 19 m 17 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2015.txt\n",
      "#--memory=1.1 MB; 21 m 12 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2016.txt\n",
      "#--memory=1.1 MB; 23 m 47 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2017.txt\n",
      "#--memory=1.1 MB; 27 m 3 s.                                                       \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2018.txt\n",
      "#--memory=1.8 MB; 34 m 6 s.                                                       \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2019.txt\n",
      "#--memory=1.9 MB; 40 m 59 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2020.txt\n",
      "#--memory=1.2 MB; 46 m 54 s.                                                      \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2021.txt\n",
      "#--memory=1.3 MB; 52 m 7 s.                                                       \n",
      "..\\data\\vectors\\fb_pol\\mt5-xl 2022.txt\n",
      "#--memory=1.3 MB; 58 m 58 s.                                                      \r"
     ]
    }
   ],
   "source": [
    "semantic_change(\n",
    "    model = Path(\"../data/vectors/fb_pol/mt5-xl/\"), \n",
    "    out_dir = Path(\"../Results/fb_pol-yearly-berT5/mT5-xl/\"), \n",
    "    n_controls = 10, \n",
    "    pool_by = centroid, \n",
    "    merge_cmp = True, # Merge compounds, e.g. N1C_berikareX --> N1_berikare (there is a risk BERT clustering picks up on this)\n",
    "    x_help = True, # Ad hoc! Should be solved in the data!! E.g. \"X_hjälpa dem på plats\" should be solved by paradigm file\n",
    "    tmp_dir = \"C:/Users/xbohma/Desktop/tmp/\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
