{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure (psuedo code)\n",
    "```\n",
    "DWEs = {förortsgäng, återvandring, globalist, berika}\n",
    "selection_strategies = {really_naive, naive_no_overlap, top1, top3, ...}\n",
    "models = {sbert_kb, ...}\n",
    "\n",
    "for DWE in DWEs:\n",
    "    for strategy in selection_strategies:\n",
    "        for model in models:\n",
    "\n",
    "            IN_vectors, OUT_vectors = select(replacement_vectors_of_dwe, strategy)\n",
    "            avg_IN_vec = mean(IN_vectors)\n",
    "            avg_OUT_vec = mean(OUT_vectors)\n",
    "\n",
    "            for year in years:\n",
    "                dwe_vect_at_year = get_vec(DWE)\n",
    "\n",
    "                IN_dimension_mean = cosine_similarity(avg_IN_vec, dwe_vect_at_year)\n",
    "                IN_dimension_pairwise_mean = mean(cosine_similarity(IN_vectors, dwe_vect_at_year))\n",
    "\n",
    "                OUT_dimension_mean = cosine_similarity(avg_OUT_vec, dwe_vect_at_year)\n",
    "                OUT_dimension_pairwise_mean = mean(cosine_similarity(OUT_vectors, dwe_vect_at_year))  \n",
    "\n",
    "                norm_dimension_mean = normalizer(IN_dimension_mean, OUT_dimension_mean)\n",
    "                norm_dimension_pariwise_mean = normalizer(IN_dimension_pairwise_mean, OUT_dimension_pairwise_mean)\n",
    "\n",
    "                # e.g. softmax or simply normalize(x, y) = x / (x+y)\n",
    "```\n",
    "\n",
    "#### Will get you something like (for each model)...\n",
    "\n",
    "|DWE            |Selection strategy|Method Dimension|Year<sub>1</sub>|...|Year<sub>*n*</sub>|\n",
    "|---------------|------------------|----------------|----------------|---|------------------|\n",
    "|DWE<sub>1</sub>|Really naive      |Mean            |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Really naive      |Pairwise mean   |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Really naive      |Normalized      |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Mean            |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Pairwise mean   |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Naive no overlap  |Normalized      |...             |...|...               |\n",
    "|DWE<sub>1</sub>|Top1              |...             |...             |...|...               |\n",
    "|...            |...               |...             |...             |...|...               |\n",
    "|DWE<sub>2</sub>|...               |...             |...             |...|...               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "from pathlib import Path\n",
    "from sklearn.utils.extmath import softmax\n",
    "from collections import Counter\n",
    "import json\n",
    "from gensim.models import KeyedVectors\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unimorph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 10:46:05 INFO: Loading these models for language: sv (Swedish):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | talbanken |\n",
      "| pos       | talbanken |\n",
      "| lemma     | talbanken |\n",
      "=========================\n",
      "\n",
      "2024-06-03 10:46:05 INFO: Use device: cpu\n",
      "2024-06-03 10:46:05 INFO: Loading: tokenize\n",
      "2024-06-03 10:46:06 INFO: Loading: pos\n",
      "2024-06-03 10:46:07 INFO: Loading: lemma\n",
      "2024-06-03 10:46:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='sv', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyness(trg, ref, min_frq = 3, verbose = True): # Consider metric\n",
    "    \n",
    "    d = dict()\n",
    "    \n",
    "#     trg_tot = sum(trg.values())\n",
    "#     ref_tot = sum(ref.values())\n",
    "    trg_tot = len(trg)\n",
    "    ref_tot = len(ref)\n",
    "    \n",
    "    for w in trg.keys():\n",
    "        if trg[w] < min_frq:\n",
    "            continue\n",
    "        if w in ref:\n",
    "            d[w] = (trg[w] / trg_tot) / (ref[w] / ref_tot) # Odds Ratio (OR)\n",
    "        else:\n",
    "            d[w] = np.inf\n",
    "    \n",
    "    if verbose:\n",
    "        for word, trg_freq, keyness  in sorted([(w, trg[w], k) for w, k in d.items()], key = lambda x: x[1], reverse = True)[:20]:\n",
    "            if word in ref:\n",
    "                ref_freq = ref[word]\n",
    "            else:\n",
    "                ref_freq = 0\n",
    "            print(f\"{word:<20}{trg_freq:<4}{(trg_freq/trg_tot):<6.3f}{ref_freq:<4}{(ref_freq/ref_tot):<6.3f}{keyness:.4}\")        \n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(\n",
    "    df,            # Replacement Dataframe\n",
    "    dwe,           # Dog Whistle Expression\n",
    "    meaning,       # 1 for ingroup, 2 for outgroup\n",
    "    phase,         # 1 for first phase of data collection, 2 for second phase\n",
    "    sw = None,     # stopwords\n",
    "    punct = None,  # remove punctuations\n",
    "    verbose = True,\n",
    "    multi = False, # Keep the multi-word units of the replacements\n",
    "    rel_freq = False, # use relative frequncies freq / no. of documents\n",
    "    lower_input = True \n",
    "):\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    if type(df) == pd.DataFrame:\n",
    "        column = df.loc[df[f\"{dwe}_w{phase}_C\"] == meaning, f\"{dwe}_text_w{phase}\"]\n",
    "    else:\n",
    "        column = df\n",
    "    \n",
    "    for x in column:\n",
    "        if lower_input:\n",
    "            x = x.lower()\n",
    "        \n",
    "        if punct != None:\n",
    "            for p in punct:\n",
    "                x = x.replace(p, \"\")\n",
    "        x = x.split()\n",
    "        if sw != None:\n",
    "            x = [w for w in x if w not in sw]\n",
    "        \n",
    "        if multi:\n",
    "            x = [\"_\".join(x)]\n",
    "        \n",
    "        counter.update(set(x)) # Obs. terms are only counted once per \"document\"\n",
    "    \n",
    "    if rel_freq:\n",
    "        counter = Counter({w: c/len(column) for w,c in counter.items()})\n",
    "        \n",
    "    if verbose:\n",
    "        for w, f in sorted(counter.items(), key = lambda x: x[1], reverse = True)[:15]:\n",
    "            print(f\"{w:<30}{f}\")\n",
    "        print(\"-----------------------\")\n",
    "        print(\"Total no. of types:\", len(counter))\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_A(\n",
    "    df,             # Replacement Dataframe\n",
    "    dwe,            # Dog Whistle Expression\n",
    "    phase = \"both\", # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "    sw = None,      # stopwords\n",
    "    punct = None,   # remove punctuations\n",
    "    k = None,\n",
    "    min_freq = None,\n",
    "    min_OR = None,\n",
    "    empty_intersect = False\n",
    "):\n",
    "    \n",
    "    if type(k) == tuple:\n",
    "        k_in, k_out = k\n",
    "    else:\n",
    "        k_in  = k\n",
    "        k_out = k\n",
    "    if type(min_freq) == tuple:\n",
    "        min_freq_in, min_freq_out = min_freq\n",
    "    else:\n",
    "        min_freq_in  = min_freq\n",
    "        min_freq_out = min_freq\n",
    "    if type(min_OR) == tuple:\n",
    "        min_OR_in, min_OR_out = min_OR\n",
    "    else:\n",
    "        min_OR_in  = min_OR\n",
    "        min_OR_out = min_OR\n",
    "    \n",
    "    if phase == \"both\":\n",
    "        x = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 1, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 1, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "                \n",
    "        y = pd.concat([\n",
    "            df.loc[df[f\"{dwe}_w{1}_C\"] == 2, f\"{dwe}_text_w{1}\"],\n",
    "            df.loc[df[f\"{dwe}_w{2}_C\"] == 2, f\"{dwe}_text_w{2}\"]\n",
    "        ]).to_list()\n",
    "\n",
    "        ingroup = inspect(x, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(y, dwe, None, None, sw, punct, verbose = False, rel_freq = True)\n",
    "\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "        \n",
    "    else:    \n",
    "    \n",
    "        ingroup = inspect(df, dwe, 1, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "        outgroup = inspect(df, dwe, 2, phase, sw, punct, verbose = False, rel_freq = True)\n",
    "        keyness_in2out = keyness(ingroup, outgroup, verbose = False, min_frq = -1)\n",
    "        keyness_out2in = keyness(outgroup, ingroup, verbose = False, min_frq = -1)\n",
    "    \n",
    "    A_in  = [w for w in ingroup.keys()]\n",
    "    A_out = [w for w in outgroup.keys()]\n",
    "    \n",
    "    if empty_intersect:\n",
    "        A_in  = [w for w in A_in if w not in outgroup.keys()]\n",
    "        A_out = [w for w in A_out if w not in ingroup.keys()]\n",
    "        \n",
    "    if min_freq != None:\n",
    "        A_in  = [w for w in A_in if ingroup[w] >= min_freq_in]\n",
    "        A_out = [w for w in A_out if outgroup[w] >= min_freq_out]\n",
    "    \n",
    "    if min_OR != None:\n",
    "        A_in  = [w for w in A_in if keyness_in2out[w] >= min_OR_in]\n",
    "        A_out = [w for w in A_out if keyness_out2in[w] >= min_OR_out] # too strict to have the same threshold for both\n",
    "        \n",
    "    if k != None:\n",
    "        A_in  = [w for w,_ in sorted(ingroup.items(), key = lambda x: x[1], reverse = True) if w in A_in][:k_in]\n",
    "        A_out = [w for w,_ in sorted(outgroup.items(), key = lambda x: x[1], reverse = True) if w in A_out][:k_out]\n",
    "    \n",
    "    \n",
    "    return A_in, A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     df,            # Replacement Dataframe\n",
    "#     dwe,           # Dog Whistle Expression\n",
    "#     phase,         # 1 for first phase of data collection, 2 for second phase, \"both\" for both\n",
    "#     sw = None,     # stopwords\n",
    "#     punct = None,  # remove punctuations\n",
    "#     k = None,\n",
    "#     min_freq = None,\n",
    "#     min_OR = None,\n",
    "#     empty_intersect = False\n",
    "\n",
    "def strat2select(mode, dwe, wh_rnds, path_dfA, stopwords, punct, verbose = True):\n",
    "    \"\"\"\n",
    "    Based on a strategy, i.e. `mode`, Select A and returns vectors of replacments that map to A. \n",
    "    Uses `select_A()` and `collect_vec()`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == \"rn\":    # Really naive; probably the most sensible for SBERT\n",
    "        \n",
    "        igt_vectors = []\n",
    "        for rnd in wh_rnds:\n",
    "            _, vecs = zip(*load_replacements(dwe, \"ingroup\", rnd, model, data_path))\n",
    "            \n",
    "            igt_vectors.extend(vecs)\n",
    "        \n",
    "        ogt_vectors = []\n",
    "        for rnd in wh_rnds:\n",
    "            _, vecs = zip(*load_replacements(dwe, \"outgroup\", rnd, model, data_path))\n",
    "            ogt_vectors.extend(vecs)  \n",
    "            \n",
    "        return np.array(igt_vectors), np.array(ogt_vectors)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        dfA = pd.read_csv(path_dfA, sep=\"\\t\") # check parameters\n",
    "        dfA = dfA.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "        \n",
    "        if wh_rnds == [\"first_round\"]:\n",
    "            PHASE = 1\n",
    "        if wh_rnds == [\"second_round\"]:\n",
    "            PHASE = 2\n",
    "        else:\n",
    "            PHASE = \"both\"\n",
    "        \n",
    "        if mode == \"nno\":   # Naive No Overlap\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe, \n",
    "                phase = PHASE, \n",
    "                sw = stopwords, \n",
    "                punct = punct, \n",
    "                empty_intersect = True)\n",
    "\n",
    "        if mode == \"top1\":  # Top 1 (no overlap)\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 1,\n",
    "                empty_intersect = True\n",
    "            )\n",
    "\n",
    "        if mode == \"top3\":  # Top 3 (no overlap)\n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 3,\n",
    "                empty_intersect = True\n",
    "            )\n",
    "\n",
    "        if mode == \"ms1\":    # Multiple Selection; threshold ... \n",
    "            Aigt, Aogt = select_A(\n",
    "                df = dfA, \n",
    "                dwe = dwe,\n",
    "                phase = PHASE,\n",
    "                sw = stopwords,\n",
    "                punct = punct,\n",
    "                k = 3,\n",
    "                min_OR = 2.0,\n",
    "                empty_intersect = False\n",
    "            )\n",
    "        \n",
    "        if verbose:\n",
    "            if len(Aigt) < 4:\n",
    "                logging.info(f\"Aigt: {', '.join(Aigt)}\")\n",
    "                logging.info(f\"Aogt: {', '.join(Aogt)}\")\n",
    "        \n",
    "        if wh_rnds == 1:\n",
    "            rounds = [\"first_round\"]\n",
    "        elif wh_rnds == 2:\n",
    "            rounds = [\"second_round\"]\n",
    "        else: # i.e. wh_rnds == \"both\"\n",
    "            rounds = [\"first_round\", \"second_round\"]\n",
    "        \n",
    "        return Aigt, Aogt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PairwiseMeanSimilarity(v, v_list, METHOD = cosine_similarity):\n",
    "    \n",
    "    pairwise = METHOD(v, v_list)\n",
    "    pairwise_mean = pairwise.mean()\n",
    "    \n",
    "    return pairwise_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_distance(v1, v2):\n",
    "    \n",
    "    angular = np.arccos(cosine_similarity(v1,v2)) / np.pi # Noble et al\n",
    "    \n",
    "    return angular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_string(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repl_dwe(dwe, rule = None, verbose = True):\n",
    "    \n",
    "    if rule != None:\n",
    "        return rule[dwe]\n",
    "    else: # infer!\n",
    "        potential_dwes = [\"forortsgang\", \"aterinvandring\", \"berikar\", \"globalister\"]\n",
    "        \n",
    "        dwe = dwe.split(\"_\")[-1]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_guess = None\n",
    "        \n",
    "        for candidate in potential_dwes:\n",
    "            score = similar_string(dwe, candidate)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_guess = candidate\n",
    "        \n",
    "        if verbose:\n",
    "            logging.info(f\"Inference for {dwe}: {best_guess} (score = {best_score:.2f}).\")\n",
    "        \n",
    "        return best_guess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self\n",
    "        \n",
    "#         self.dwes = dwes\n",
    "#         self.wh_rounds = rounds\n",
    "#         self.dfA_path = dfA_path\n",
    "#         self.strategies = strategies\n",
    "#         self.years = years\n",
    "#         self_measures = \n",
    "#         self.add_correlations\n",
    "#         self.model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyed_vec(term, keyed_vecs):\n",
    "    \n",
    "    if term in keyed_vecs:\n",
    "        vec = keyed_vecs[term]\n",
    "    else:\n",
    "        vec = None\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(filename): \n",
    "    vocab = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f.readlines():\n",
    "            w,frq = line.rstrip('\\n').split()\n",
    "            vocab[w] = int(frq)\n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(A):\n",
    "    \"\"\" \n",
    "    Lemmatizes a list of words. \n",
    "    If no lemma is found, the original term is kept as the lemma.\n",
    "    \"\"\"\n",
    "    \n",
    "    A_mod     = {}\n",
    "    for w in A:\n",
    "        doc = nlp(w)\n",
    "        lemma = doc.sentences[0].words[0].lemma # it seems stanza always returns something\n",
    "        \n",
    "        ######################################################################\n",
    "        # The Stanza lemmatizer for Swedish infer the lemma \"jud\" for \"judar\"?\n",
    "        # How to \"update\" the lemmatizer with lemma \"jude\" for \"judar\"\n",
    "        if lemma == \"jud\":\n",
    "            lemma = \"jude\"\n",
    "        ######################################################################\n",
    "        \n",
    "        if lemma in A_mod:\n",
    "            A_mod[lemma].add(w)\n",
    "        else:\n",
    "            A_mod[lemma] = set()\n",
    "            A_mod[lemma].add(w)\n",
    "    \n",
    "    return list(A_mod.keys()), A_mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wf_expand(B, A_mod, use_saldo, saldo, verbose = True):\n",
    "    \"\"\" \n",
    "    Expands a lemma (lexeme) to its word forms. \n",
    "    If no expansion is found, the lemma form + the original word form(s) re kept as the only word forms.\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_B = []\n",
    "    \n",
    "    for lemma in B:\n",
    "        # Try first unimorph\n",
    "        wfs = [line.split(\"\\t\")[1] for line in unimorph.inflect_word(lemma, lang=\"swe\").split(\"\\n\") if line != \"\"]\n",
    "        if wfs != []:\n",
    "            exp_B.append(set(wfs))\n",
    "        else: # wfs == []\n",
    "            if use_saldo:\n",
    "                # Try Saldo\n",
    "                if lemma in saldo:\n",
    "                    exp_B.append(set(saldo[lemma]))\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        logging.info(f\"For {lemma}, neither `unimorph` nor `saldo` found nothing.\")\n",
    "                    logging.info(f\"Amod to rescue...{set(A_mod[lemma])}\")\n",
    "                    exp_B.append(set(A_mod[lemma]))\n",
    "            else:\n",
    "                exp_B.append(set(A_mod[lemma]))\n",
    "        \n",
    "            \n",
    "    return exp_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2b2vec(A, strategy, wv, vocab, use_saldo, saldo):\n",
    "    \n",
    "    vecs = []             \n",
    "    \n",
    "    if strategy == \"lazy\": # do nothing; take them as they are\n",
    "        \n",
    "        for w in [w for w in A if w in wv]:\n",
    "            vecs.append(wv[w])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        B, A_mod = lemmatize(A)\n",
    "        B        = wf_expand(B, A_mod, use_saldo, saldo) # implement as a list of sets in order to pick to most common forms of a lemma\n",
    "        B        = [[w for w in lexeme if w in wv and w in vocab] for lexeme in B]\n",
    "        \n",
    "        \n",
    "        if strategy == \"greedy\": # hungry\n",
    "            logging.info(f\"B: {B}\")\n",
    "            for lexeme in B:\n",
    "                for w in lexeme:\n",
    "                    vecs.append(wv[w])\n",
    "                    \n",
    "        else:\n",
    "            #print(B)\n",
    "            lemmatized_voc_B   = {lexeme[0].upper(): {w: vocab[w] for w in lexeme if w in vocab} for lexeme in B if not lexeme == []}\n",
    "            flattened_voc_B    = {wf: vocab[wf] for lexeme in B for wf in lexeme}\n",
    "            proportional_voc_B = {lexeme: {w: (lemmatized_voc_B[lexeme][w]/sum(lemmatized_voc_B[lexeme].values())) for w in lemmatized_voc_B[lexeme]} for lexeme in lemmatized_voc_B}\n",
    "            flatt_prop_voc_B   = dict()\n",
    "            for p_dict in proportional_voc_B.values():\n",
    "                for w, prop in p_dict.items():\n",
    "                    flatt_prop_voc_B[w] = prop\n",
    "            \n",
    "            if strategy.startswith(\"top\"): # e.g. top1, top3, etc.\n",
    "                k = int(strategy.replace(\"top\", \"\"))\n",
    "                T = []\n",
    "\n",
    "                for lexeme in B: # lexeme is a list\n",
    "                    \n",
    "                    VOC = {w:f for w,f in flattened_voc_B.items() if w in lexeme}\n",
    "                    \n",
    "                    ranked = sorted(VOC.items(), key = lambda x: x[1], reverse=True)[:k]\n",
    "                    for w, _ in ranked:\n",
    "                        T.append(w)\n",
    "\n",
    "                logging.info(f\"B: {', '.join(T)}\")\n",
    "                for w in T:\n",
    "                    vecs.append(wv[w])\n",
    "            \n",
    "            if strategy.startswith(\"min\"): # e.g. min0.1 \n",
    "                threshold = float(strategy.replace(\"min\", \"\"))\n",
    "                T = []\n",
    "            \n",
    "                for lexeme in B:\n",
    "                    for w in lexeme:\n",
    "                        if flatt_prop_voc_B[w] >= threshold:\n",
    "                            T.append(w)\n",
    "                            \n",
    "                logging.info(f\"B: {', '.join(T)}\")                \n",
    "                for w in T:\n",
    "                    vecs.append(wv[w])\n",
    "    \n",
    "    if vecs == []:\n",
    "        return None\n",
    "    else:\n",
    "        \n",
    "        return np.array(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgns_builder(config):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        handlers=[\n",
    "            logging.FileHandler(config.log_path / f\"{config.log_prefix}_sgns_builder.log\", mode= \"w\"),\n",
    "            logging.StreamHandler()\n",
    "        ])\n",
    "    \n",
    "    logging.info(vars(config))    \n",
    "    results = []\n",
    "    methods = [\n",
    "        \"I-cnt\", \n",
    "        \"I-ang\",\n",
    "        \"I-pwn\", \n",
    "        \"O-cnt\",\n",
    "        \"O-pwn\",\n",
    "        \"O-ang\",\n",
    "        \"cnt-ssc\", \n",
    "        \"cnt-smx\", \n",
    "        \"pwn-ssc\", \n",
    "        \"pwn-smx\",\n",
    "        \"ang-ssc\",\n",
    "        \"ang-smx\",\n",
    "        \"sep-ang\",\n",
    "        \"sep-euc\"\n",
    "        ]\n",
    "    # alternative: make this a config attribute\n",
    "    years = [str(year) for year in range(config.first_year, config.last_year+1)] # Obs! need to add 1\n",
    "    years.sort()\n",
    "    \n",
    "    if config.use_saldo:\n",
    "        with open(config.saldo_path) as f:\n",
    "            saldo = json.loads(f.read())\n",
    "    else:\n",
    "        saldo = None\n",
    "    \n",
    "    with open(config.stopwords) as f:\n",
    "        stopwords = [w.strip(\"\\n\") for w in f.readlines()]\n",
    "    \n",
    "    for progress, dwe in enumerate(config.dwes, start = 1):\n",
    "        t = time.time() - t0\n",
    "        logging.info(f\"PROCESSING {progress} OF {len(config.dwes)}: '{dwe}'; {int(t/60)} m. {int(t%60)} s.\")\n",
    "        dwe_in_replacement_test = repl_dwe(dwe)\n",
    "        for a_strategy in config.Astrategies:\n",
    "            logging.info(f\"A-Strategy: {a_strategy}\")\n",
    "            \n",
    "            Aigt, Aogt = strat2select(\n",
    "                mode      = a_strategy, \n",
    "                dwe       = dwe_in_replacement_test, \n",
    "                wh_rnds   = config.wh_rounds, \n",
    "                #model     = config.model, \n",
    "                path_dfA  = config.dfA_path, \n",
    "                stopwords = stopwords, \n",
    "                punct     = config.punct, \n",
    "                #data_path = config.data_path\n",
    "            )\n",
    "\n",
    "            d = {b: {method: [] for method in methods} for b in config.Bstrategies}\n",
    "\n",
    "            for year in years:\n",
    "                \n",
    "                wv = KeyedVectors.load_word2vec_format(config.sgns_path / f\"{year}.w2v\")\n",
    "                vocab = load_vocab(config.vocab_path / f\"{year}.txt\")\n",
    "\n",
    "                dwe_vector = get_keyed_vec(dwe, wv)\n",
    "                \n",
    "                for b_strategy in config.Bstrategies:\n",
    "                    logging.info(f\"{year} :: B-strategy: {b_strategy}\")\n",
    "                \n",
    "                    if type(dwe_vector) != np.ndarray:\n",
    "                        d[b_strategy][\"I-cnt\"].append(None) \n",
    "                        d[b_strategy][\"O-cnt\"].append(None)\n",
    "                        d[b_strategy][\"I-pwn\"].append(None)\n",
    "                        d[b_strategy][\"O-pwn\"].append(None)                        \n",
    "                        d[b_strategy][\"I-ang\"].append(None)\n",
    "                        d[b_strategy][\"O-ang\"].append(None)\n",
    "                        \n",
    "                        d[b_strategy][\"cnt-ssc\"].append(None)\n",
    "                        d[b_strategy][\"cnt-smx\"].append(None)\n",
    "                        d[b_strategy][\"pwn-ssc\"].append(None)\n",
    "                        d[b_strategy][\"pwn-smx\"].append(None) \n",
    "                        d[b_strategy][\"ang-ssc\"].append(None)\n",
    "                        d[b_strategy][\"ang-smx\"].append(None)\n",
    "\n",
    "                        d[b_strategy][\"sep-ang\"].append(None)\n",
    "                        d[b_strategy][\"sep-euc\"].append(None)                        \n",
    "\n",
    "                    else:                \n",
    "                        logging.info(\"In-group\")\n",
    "                        INGROUPvec  = a2b2vec(Aigt, b_strategy, wv, vocab, config.use_saldo, saldo)\n",
    "                        logging.info(\"Out-group\")\n",
    "                        OUTGROUPvec = a2b2vec(Aogt, b_strategy, wv, vocab, config.use_saldo, saldo)\n",
    "                        \n",
    "                        if type(INGROUPvec) == np.ndarray:\n",
    "                            ING_centroid  = INGROUPvec.mean(axis=0)\n",
    "                            \n",
    "                            i_cnt = cosine_similarity(\n",
    "                                dwe_vector.reshape(1,-1), \n",
    "                                ING_centroid.reshape(1,-1)\n",
    "                                )[0][0]\n",
    "                            i_pwn = PairwiseMeanSimilarity(dwe_vector.reshape(1, -1), INGROUPvec)\n",
    "                            i_cnt_ang = 1 - angular_distance(\n",
    "                                dwe_vector.reshape(1,-1), \n",
    "                                ING_centroid.reshape(1,-1)\n",
    "                                )[0][0] # https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/cosdist.htm\n",
    "                            \n",
    "                            d[b_strategy][\"I-cnt\"].append(i_cnt)\n",
    "                            d[b_strategy][\"I-pwn\"].append(i_pwn)\n",
    "                            d[b_strategy][\"I-ang\"].append(i_cnt_ang)\n",
    "                            \n",
    "                            if type(OUTGROUPvec) == np.ndarray: # Both ingroup and outgroup vector\n",
    "                                OUTG_centroid = OUTGROUPvec.mean(axis=0)\n",
    "\n",
    "                                o_cnt = cosine_similarity(dwe_vector.reshape(1,-1), OUTG_centroid.reshape(1,-1))[0][0]\n",
    "                                o_pwn = PairwiseMeanSimilarity(dwe_vector.reshape(1, -1), OUTGROUPvec)\n",
    "                                o_cnt_ang = 1 - angular_distance(\n",
    "                                    dwe_vector.reshape(1,-1), \n",
    "                                    OUTG_centroid.reshape(1,-1)\n",
    "                                    )[0][0] # https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/cosdist.htm\n",
    "\n",
    "                                sep_ang_cnt = angular_distance(\n",
    "                                    ING_centroid.reshape(1,-1), \n",
    "                                    OUTG_centroid.reshape(1,-1)\n",
    "                                    )[0][0] \n",
    "                                sep_euc_cnt = euclidean_distances(\n",
    "                                    ING_centroid.reshape(1,-1), \n",
    "                                    OUTG_centroid.reshape(1,-1)\n",
    "                                    )[0][0]\n",
    "                                \n",
    "                                d[b_strategy][\"O-cnt\"].append(o_cnt)\n",
    "                                d[b_strategy][\"O-pwn\"].append(o_pwn)\n",
    "                                d[b_strategy][\"O-ang\"].append(o_cnt_ang)\n",
    "                                 \n",
    "                                d[b_strategy][\"cnt-ssc\"].append(i_cnt / (i_cnt + o_cnt))\n",
    "                                d[b_strategy][\"cnt-smx\"].append(softmax([[i_cnt, o_cnt]])[0][0])\n",
    "                                d[b_strategy][\"pwn-ssc\"].append(i_pwn / (i_pwn + o_pwn))\n",
    "                                d[b_strategy][\"pwn-smx\"].append(softmax([[i_pwn, o_pwn]])[0][0])\n",
    "                                d[b_strategy][\"ang-ssc\"].append(i_cnt_ang / (i_cnt_ang + o_cnt_ang))\n",
    "                                d[b_strategy][\"ang-smx\"].append(softmax([[i_cnt_ang, o_cnt_ang]])[0][0])\n",
    "                                \n",
    "                                d[b_strategy][\"sep-ang\"].append(sep_ang_cnt)\n",
    "                                d[b_strategy][\"sep-euc\"].append(sep_euc_cnt)\n",
    "                            \n",
    "                            else: # Ingroup, but no outgroup vector\n",
    "                                d[b_strategy][\"O-cnt\"].append(None)\n",
    "                                d[b_strategy][\"O-pwn\"].append(None)\n",
    "                                d[b_strategy][\"O-ang\"].append(None)\n",
    "                                \n",
    "                                d[b_strategy][\"cnt-ssc\"].append(None)\n",
    "                                d[b_strategy][\"cnt-smx\"].append(None)\n",
    "                                d[b_strategy][\"pwn-ssc\"].append(None)\n",
    "                                d[b_strategy][\"pwn-smx\"].append(None)\n",
    "                                d[b_strategy][\"ang-ssc\"].append(None)\n",
    "                                d[b_strategy][\"ang-smx\"].append(None)\n",
    "\n",
    "                                d[b_strategy][\"sep-ang\"].append(None)\n",
    "                                d[b_strategy][\"sep-euc\"].append(None)                                \n",
    "                                \n",
    "                        else: # No ingroup vector\n",
    "                            d[b_strategy][\"I-cnt\"].append(None)\n",
    "                            d[b_strategy][\"I-pwn\"].append(None)\n",
    "                            d[b_strategy][\"I-ang\"].append(None)\n",
    "                            \n",
    "                            d[b_strategy][\"cnt-ssc\"].append(None)\n",
    "                            d[b_strategy][\"cnt-smx\"].append(None)\n",
    "                            d[b_strategy][\"pwn-ssc\"].append(None)\n",
    "                            d[b_strategy][\"pwn-smx\"].append(None)\n",
    "                            d[b_strategy][\"ang-ssc\"].append(None)\n",
    "                            d[b_strategy][\"ang-smx\"].append(None)\n",
    "                            \n",
    "                            d[b_strategy][\"sep-ang\"].append(None)\n",
    "                            d[b_strategy][\"sep-euc\"].append(None)                                \n",
    "                            \n",
    "                            if type(OUTGROUPvec) == np.ndarray: # Outgtoup, but no ingroup vector\n",
    "                                OUTG_centroid = OUTGROUPvec.mean(axis=0)\n",
    "                                \n",
    "                                o_cnt = cosine_similarity(\n",
    "                                    dwe_vector.reshape(1,-1), \n",
    "                                    OUTG_centroid.reshape(1,-1)\n",
    "                                    )[0][0]\n",
    "                                o_pwn = PairwiseMeanSimilarity(dwe_vector.reshape(1, -1), OUTGROUPvec)\n",
    "                                o_cnt_ang = 1 - angular_distance(\n",
    "                                    dwe_vector.reshape(1,-1), \n",
    "                                    OUTG_centroid.reshape(1,-1)\n",
    "                                    )[0][0] # https://www.itl.nist.gov/div898/software/dataplot/refman2/auxillar/cosdist.htm\n",
    "\n",
    "                                d[b_strategy][\"O-cnt\"].append(o_cnt)\n",
    "                                d[b_strategy][\"O-pwn\"].append(o_pwn)\n",
    "                                d[b_strategy][\"O-ang\"].append(o_cnt_ang)\n",
    "\n",
    "            if config.results_format == \"long\":\n",
    "                \n",
    "                for b_strategy in d.keys():\n",
    "                    for method in d[b_strategy].keys():\n",
    "                        line = [dwe, a_strategy, b_strategy, method]\n",
    "                        line.extend(d[b_strategy][method])\n",
    "    #                     if config.add_correlations:\n",
    "    #                         r_naive\n",
    "    #                         r_rect\n",
    "    #                         rho_naive\n",
    "    #                         rho_rect\n",
    "    #                         r_fpm\n",
    "    #                         rho_fpm\n",
    "    #                         N                    \n",
    "                        results.append(line)\n",
    "\n",
    "            else: # if results_format == \"wide\"\n",
    "                for b_strategy in d.keys():\n",
    "                    line = [dwe, a_strategy, b_strategy]\n",
    "                    for method in d[b_strategy].keys:\n",
    "                        line.extend(d[b_strategy][method])\n",
    "                results.append(line)\n",
    "    \n",
    "    if config.results_format == \"long\":\n",
    "        features = [\"DWE\", \"A-Strategy\", \"B-Strategy\", \"Method\"] + years\n",
    "        if config.add_correlations:\n",
    "            additional_headings = [\"r_naive\", \"r_rect\", ...]\n",
    "            features.extend(additional_headings)\n",
    "        \n",
    "    \n",
    "    else: # if wide\n",
    "        features = [\"DWE\", \"A-Strategy\", \"B-Strategy\"]\n",
    "        for method in methods:\n",
    "            m = [f\"{method}_{year}\" for year in years]\n",
    "            features.extend(m)\n",
    "    \n",
    "    df = pd.DataFrame(results, columns = features)\n",
    "    \n",
    "    df.to_csv(config.results_path)\n",
    "    \n",
    "    t = time.time() - t0\n",
    "    logging.info(f\"Done! {int(t/60)} m. {int(t%60)} s.\")    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window sizes, 100 dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w5, 10, 15 (Flashback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for WS in [5, 10, 15]:\n",
    "    config = Config()\n",
    "    #WS=5\n",
    "\n",
    "    config.log_prefix = f\"fb-w{WS}\"\n",
    "    config.log_path   = Path(f\"/home/max/Results/rplc_w{WS}/log\")\n",
    "    config.first_year = 2000\n",
    "    config.last_year  = 2022\n",
    "    #config.last_year  = 2005\n",
    "    config.dwes       = [\n",
    "                        \"V1_berika\",\n",
    "                        \"N1_berikare\",\n",
    "                        \"V1_kulturberika\",\n",
    "                        \"N1_kulturberikare\",\n",
    "                        \"N1_globalist\",\n",
    "                        \"A1_globalistisk\",\n",
    "                        \"N1_återvandring\",\n",
    "                        \"V1_återvandra\",\n",
    "                        #\"V1_hjälpa_på_plats\",\n",
    "                        \"N1_förortsgäng\"\n",
    "                        ]\n",
    "    #config.Astrategies = [\"top1\", \"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    config.Astrategies = [\"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    #config.Bstrategies = [\"lazy\", \"greedy\", \"top1\", \"top3\", \"min0.5\", \"min0.2\"]\n",
    "    config.Bstrategies = [\"lazy\", \"greedy\", \"top3\", \"min0.2\"]\n",
    "    config.wh_rounds  = [\"first_round\", \"second_round\"]\n",
    "    config.dfA_path   = Path(\"/home/max/Documents/research/replacement_data/panel_wide_onlyreplace.csv\")\n",
    "    config.stopwords  = Path(\"../../data/utils/stopwords-sv.txt\")\n",
    "    config.punct      = [\",\", \"?\", \".\", \"!\", \";\", \"”\", '\"', \")\", \")\", \"&\", \"=\", \"'\"]\n",
    "#     config.data_path   = Path(f\"/home/max/Results/rplc_w{WS}/data\")\n",
    "    config.results_format = \"long\"\n",
    "    config.add_correlations = False\n",
    "    config.results_path = Path(f\"/home/max/Results/rplc_w{WS}/results/fb_sgns-w{WS}_results.csv\")\n",
    "    config.sgns_path = Path(f\"/home/max/Results/fb_pol-yearly-rad3-w{WS}-d100/models\") # <- 100 dim!\n",
    "    config.use_saldo = True\n",
    "    config.saldo_path = Path(\"/home/max/Datasets/saldom.json\")\n",
    "    config.vocab_path = Path(\"/home/max/Corpora/flashback-pol-time/yearly/fb-pt-radical3/vocab\")\n",
    "\n",
    "    sgns_builder(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window sizes, 200 dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w5, 10, 15 (Flashback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for WS in [5]:\n",
    "    config = Config()\n",
    "    #WS=5\n",
    "\n",
    "    config.log_prefix = f\"fb-w{WS}-200\"\n",
    "    config.log_path   = Path(f\"/home/max/Results/rplc_w{WS}-200/log\")\n",
    "    config.first_year = 2000\n",
    "    config.last_year  = 2022\n",
    "    #config.last_year  = 2005\n",
    "    config.dwes       = [\n",
    "                        \"V1_berika\",\n",
    "                        \"N1_berikare\",\n",
    "                        \"V1_kulturberika\",\n",
    "                        \"N1_kulturberikare\",\n",
    "                        \"N1_globalist\",\n",
    "                        \"A1_globalistisk\",\n",
    "                        \"N1_återvandring\",\n",
    "                        \"V1_återvandra\",\n",
    "                        #\"V1_hjälpa_på_plats\",\n",
    "                        \"N1_förortsgäng\"\n",
    "                        ]\n",
    "    #config.Astrategies = [\"top1\", \"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    config.Astrategies = [\"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    #config.Bstrategies = [\"lazy\", \"greedy\", \"top1\", \"top3\", \"min0.5\", \"min0.2\"]\n",
    "    config.Bstrategies = [\"lazy\", \"greedy\", \"top3\", \"min0.2\"]\n",
    "    config.wh_rounds  = [\"first_round\", \"second_round\"]\n",
    "    config.dfA_path   = Path(\"/home/max/Documents/research/replacement_data/panel_wide_onlyreplace.csv\")\n",
    "    config.stopwords  = Path(\"../../data/utils/stopwords-sv.txt\")\n",
    "    config.punct      = [\",\", \"?\", \".\", \"!\", \";\", \"”\", '\"', \")\", \")\", \"&\", \"=\", \"'\"]\n",
    "#     config.data_path   = Path(f\"/home/max/Results/rplc_w{WS}/data\")\n",
    "    config.results_format = \"long\"\n",
    "    config.add_correlations = False\n",
    "    config.results_path = Path(f\"/home/max/Results/rplc_w{WS}-200/results/fb_sgns-w{WS}-200_results.csv\")\n",
    "    config.sgns_path = Path(f\"/home/max/Results/fb_pol-yearly-rad3-w{WS}-d200/models\") # <- 100 dim!\n",
    "    config.use_saldo = True\n",
    "    config.saldo_path = Path(\"/home/max/Datasets/saldom.json\")\n",
    "    config.vocab_path = Path(\"/home/max/Corpora/flashback-pol-time/yearly/fb-pt-radical3/vocab\")\n",
    "\n",
    "    sgns_builder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for WS in [10, 15]:\n",
    "    config = Config()\n",
    "    #WS=5\n",
    "\n",
    "    config.log_prefix = f\"fb-w{WS}-200\"\n",
    "    config.log_path   = Path(f\"/home/max/Results/rplc_w{WS}-200/log\")\n",
    "    config.first_year = 2000\n",
    "    config.last_year  = 2022\n",
    "    #config.last_year  = 2005\n",
    "    config.dwes       = [\n",
    "                        \"V1_berika\",\n",
    "                        \"N1_berikare\",\n",
    "                        \"V1_kulturberika\",\n",
    "                        \"N1_kulturberikare\",\n",
    "                        \"N1_globalist\",\n",
    "                        \"A1_globalistisk\",\n",
    "                        \"N1_återvandring\",\n",
    "                        \"V1_återvandra\",\n",
    "                        #\"V1_hjälpa_på_plats\",\n",
    "                        \"N1_förortsgäng\"\n",
    "                        ]\n",
    "    #config.Astrategies = [\"top1\", \"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    config.Astrategies = [\"top3\", \"ms1\"] # add \"rn\", \"nno\" but need to fix code in function\n",
    "    #config.Bstrategies = [\"lazy\", \"greedy\", \"top1\", \"top3\", \"min0.5\", \"min0.2\"]\n",
    "    config.Bstrategies = [\"lazy\", \"greedy\", \"top3\", \"min0.2\"]\n",
    "    config.wh_rounds  = [\"first_round\", \"second_round\"]\n",
    "    config.dfA_path   = Path(\"/home/max/Documents/research/replacement_data/panel_wide_onlyreplace.csv\")\n",
    "    config.stopwords  = Path(\"../../data/utils/stopwords-sv.txt\")\n",
    "    config.punct      = [\",\", \"?\", \".\", \"!\", \";\", \"”\", '\"', \")\", \")\", \"&\", \"=\", \"'\"]\n",
    "#     config.data_path   = Path(f\"/home/max/Results/rplc_w{WS}/data\")\n",
    "    config.results_format = \"long\"\n",
    "    config.add_correlations = False\n",
    "    config.results_path = Path(f\"/home/max/Results/rplc_w{WS}-200/results/fb_sgns-w{WS}-200_results.csv\")\n",
    "    config.sgns_path = Path(f\"/home/max/Results/fb_pol-yearly-rad3-w{WS}-d200/models\") # <- 100 dim!\n",
    "    config.use_saldo = True\n",
    "    config.saldo_path = Path(\"/home/max/Datasets/saldom.json\")\n",
    "    config.vocab_path = Path(\"/home/max/Corpora/flashback-pol-time/yearly/fb-pt-radical3/vocab\")\n",
    "\n",
    "    sgns_builder(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
